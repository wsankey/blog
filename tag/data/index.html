<!DOCtype html>
<html>
<head>
  <meta charset="utf-8">
  <title>Will Sankey | Data and code</title>
   <script src="//use.typekit.net/puo2mkm.js"></script>
	<script>try{Typekit.load({ async: true });}catch(e){}</script>
    <link rel=stylesheet type=text/css href="/static/css/bootstrap.min.css"/>
    <script src="//code.jquery.com/jquery-2.1.4.min.js"></script>
    <script src="//netdna.bootstrapcdn.com/bootstrap/3.3.2/js/bootstrap.min.js"></script>
    <link rel=stylesheet type=text/css href="/static/css/main.css"/>
    <meta name="viewport" content="width=device-width">
</head>
<body>
<div class="container">
	<div class="row">
		<div class="col-sm-12">
  <h1><a class="brandcolor" href="/">Will Sankey</a>
  <small>
  <span class="namegroup">
  <ul id="namelist">
    <li id="list"><a class="brandcolor2" href="/aboutme/">[About,</a></li>
    <li id="list"><a class="brandcolor2" href="/posts/">Blog,</a></li>
    <li id="list"><a  class="brandcolor2"href="/projects/">Etc.]</a></h3></li>
  </ul>
  </span>
  </small>
  </h1>


<div class="container">
	<div class="row">
		<div  class="col-xs-12 card" style="margin-bottom: 4em;">
	<h1>Posts tagged: <em>data</em></h1>
	
		

<div class="container" >
	<div class="row">
		<div class="col-sm-12" style="padding: 0 2em 1em 0;">
			<span></span></h2><a style="font-size: 25px" href="/posts/2015-03-24-Echo-Chambers/">Echo Chambers</a></h2><br>

		<div class="brandcolor2" id="datetag">
		<p> Date: 2015-03-24
	
		|  Tagged:
		
			<em><a href="/tag/network/">network</a></em>
		
			<em><a href="/tag/data/">data</a></em>
		
		</p>
		<h4>“If you live near a Whole Foods,” he said. “If no relative of yours serves in the military; If you’re paid by the year, not the hour; If no one you know uses meth; If you married once and remain married; If most people you know finished college; If you aren’t one of 65 million Americans with a criminal record. If any or all of these things describe you, then accept the possibility that, actually, you may not know what’s going on, and you may be part of the problem.”</h4>
<p><em>Anand Giridharadas</em></p>
<p>I am regularly impressed by twitter. Tens of millions of people across
the world communicating together on one platform. Ideas, opinions, and
perspectives pour into the platform at all times of day. It's easy to
find your community online, and it's easy to participate in the
discussion.</p>
<p>There is another twitter, and perhaps others across the world. The one I
have in mind is called Sina-Weibo and it caters to the Eastern world,
particularly China.</p>
<p>These two twitters rarely, if ever, interact. There is the obvious
barrier of language but that is something we can overcome from both
sides with the tools of online translation. Yet it is hard to imagine
these two worlds combining into one. And so we in the West will suffer
from limited access to those ideas and perspectives from the East...at
least for a little bit longer.</p>
<h3>Have you mapped your social network, do you know the makeup of it's participants?</h3>
<p>I have the sneaking suspicion that most of our social networks are filled with people who for ook and think like us. Are in a similar age, demographic,
and socioeconomic bracket. Perhaps unknowingly we drift towards people
who represent who we are--it's comfortable, but it's limiting our perspectives.</p>
<p>And this has potentially serious outcomes, certainly from limiting our
cultural and personal growth but more insidiously, as a society, we recycle
the same solutions to problems we face. We ignore problems others are
facing because we are ignorant of them. We construct our realities from
the information we receive in part from our peers.</p>
<h3>The key idea here is that insular communities tend to recycle their ideas.</h3>
<ul>
<li><a href="http://www.pewinternet.org/2014/02/20/mapping-twitter-topic-networks-from-polarized-crowds-to-community-clusters/">The Pew Research Center found twitter users found clusters of
  political converations where individuals do not engage other ideas</a>;</li>
<li><a href="http://www.pewinternet.org/2014/02/20/mapping-twitter-topic-networks-from-polarized-crowds-to-community-clusters/">We become unaware of the wider economic realities surrounding us</a>;</li>
<li><a href="http://www.businessinsider.com/stupid-iphone-apps-2011-6">We come up with solutions to our own problems when we might be able to
  use our talents elsewhere.</a></li>
</ul>
<p>It is happening in our political sphere (the likeliest candidates for our next presidential election are Clinton and Bush). It has existed in our neighborhoods. And requires
purposefully action to combat.</p>
		</div>
			</div>
		</div>
	</div>
	

<div class="container" >
	<div class="row">
		<div class="col-sm-12" style="padding: 0 2em 1em 0;">
			<span></span></h2><a style="font-size: 25px" href="/posts/2015-03-30-ProjectBasedLearning/">Project Based Learning</a></h2><br>

		<div class="brandcolor2" id="datetag">
		<p> Date: 2015-03-30
	
		|  Tagged:
		
			<em><a href="/tag/network/">network</a></em>
		
			<em><a href="/tag/data/">data</a></em>
		
		</p>
		<h3>After learning the basics embrace the baptism by fire</h3>
<p>The most frustrating and rewarding class I ever took occurred during my first semester of
my master's program and it was centered around a project. The idea for
the class, in addition to refining our writing techniques through
grueling memoranda, was to throw students into the deep end of a policy problem. We were directed to identify the conditions of
three distinct neighborhoods around Baltimore city before and after the
recession and determine how they fared.</p>
<p>Like most projects, the problem statement was innocuous enough but the execution proved a
bear. Like most innocuous problem statements this one hid layers of
competing findings, frustrating collaboration and coordination with
peers, and required making difficult decisions on design and procedure.
It kept me and my teammates up late nights, it was the cause of a few
breakdowns, and it softened our respective freshman ideas about graduate
school.</p>
<p>Experienced workers know these feelings and probably remember their
first tango working with a difficult problem on a team of competing
personalities. But for those who don't, know this:</p>
<h3>Although it is extrememly trying at times, this is the fastest and most complete way to learn.</h3>
<p>If you are hoping to become a data scientist or experienced developer, work on related
projects. Here are some ideas:</p>
<ul>
<li><a href="http://www.newcoder.io">Python Tutorials</a></li>
<li><a href="http://www.realpython.com">Data Science and Analysis Tutorials (with Python)</a></li>
</ul>
<h3>You need to get started, right away.</h3>
<p><em>Do what you can, with what you have, where you are</em>
~Roosevelt</p>
		</div>
			</div>
		</div>
	</div>
	

<div class="container" >
	<div class="row">
		<div class="col-sm-12" style="padding: 0 2em 1em 0;">
			<span></span></h2><a style="font-size: 25px" href="/posts/2015-03-10-FreelyPublicly-Available/">Freely, Publicly Available</a></h2><br>

		<div class="brandcolor2" id="datetag">
		<p> Date: 2015-03-10
	
		|  Tagged:
		
			<em><a href="/tag/data/">data</a></em>
		
		</p>
		<h3>The flood of Open Data offers great opportunities for innovation</h3>
<p>But let's not mistake data availability for wisdom.
<img alt="open data census reporter" src="/assets/media/censusreporter.png" /></p>
<p><em>The above screenshot comes from CensusReporter.org for the College Park, MD area.</em></p>
<h2>Current Efforts at Open Data</h2>
<p>Admirably local, state, and the Federal government are making attempts at releasing massive amounts of data to the public. Living and working in the Washington Metropolitan area 've been lucky to be exposed to the leading edge of open data. For example, <a href="https://data.baltimorecity.gov/">I am very proud of the efforts made by Baltimore City to open their data catalog to citizens.</a> The city is dedicated to making information about homeless shelters, properties, city markets, crime and a slew of other metrics widely available. On the state level we have <a href="http://www.statestat.maryland.gov/">StateStat</a> and the <a href="https://data.maryland.gov/">open Maryland data portal.</a> And on the Federal level various agencies have been attempting to open their data to the public (per the image above) with Data.Gov leading the charge.</p>
<iframe width="560" height="315" src="https://www.youtube.com/embed/vbb-AjiXyh0?list=PL055Epbe6d5aWZSOZAZ4MX5xXKEvlT6y_" frameborder="0" allowfullscreen></iframe>

<p><br>
Private companies are also releasing data to the public, but understandably on a much more limited basis. <a href="https://dev.twitter.com/rest/public/rate-limiting">Twitter has limited the number of requests that can be pulled from their API</a> while <a href="http://www.buzzfeed.com/bensmith/uber-executive-suggests-digging-up-dirt-on-journalists#.tdem9rNXx">other companies, like Uber, have stumbled in their stewardship.</a> </p>
<h2>Open Data's Potential</h2>
<p>This abundant open data coupled with increased processing power are birthing new innovative efforts by citizens and organizations. For example, here in the D.C. area we have organizations such as <a href="http://www.codefordc.org"><strong>Code for DC</strong></a> using open data to achieve public goals; one project in particular is helping <a href="http://codefordc.github.io/districthousing/">accelerate the affordable housing application process.</a> Whole organizations have been initiated and augmented with this open data. District Data Labs has sprung up to help parse data while organizations such as the Sunlight Foundation use it to further their efforts. Journalists at Vox.com and Governing are using it to investigate topics like <a href="http://www.governing.com/topics/urban/gov-washington-affordable-housing-protections-gentrification-series.html">gentrification in the D.C. area.</a></p>
<p>In addition to these efforts Open Data can help revolutionize research fields. As an example, imagine an open repository site for all research papers that are accepted after peer review (a Github for academics, if you will). Non-sensitive data and the author's code could be posted so students and the public could review and replicate studies. We might be able to catch falsified reports earlier and halt their negative repercussions (e.g. falsified data linking autism to vaccinations). Other research projects could be crowdsourced and studies that take years to conduct could be bolstered by the labor of the enthused, just as projects are bolstered by contributors on Github.</p>
<h2>Power to the powerful, resistance from the entrenched</h2>
<p><em>The government is very keen on amassing statistics. They collect them, add them, raise them to the n-th power, take the cube root and prepare wonderful diagrams. But you must never forget that every one of these figures comes in the first instance from the village watchman, who just puts down what he damn pleases.</em>
--Comment of an English judge on the subject of Indian statistics; Quoted in Sir Josiah Stamp in "Some Economic Matters in Modern Life"</p>
<p>Even with increasing availability there are drawbacks and resistance to making everything open.</p>
<p>First, the rise of open data chiefly benefits those who can (1) access and process the data; (2) have the capacity to act on it. At the very least a person needs to be able to read a CSV file into a program like Excel, know how to transform it to become analysis ready and then (usually) perform a statistical analysis on it. Even for trained professionals it's sometimes hard to know where to begin, so what hope does the amateur data scientist have in understanding how to interpret the endless streams of data in front of them. It's easy to give up.</p>
<h3>This wealth of information offers power to those with the skill and capacity to interpret and act on it.</h3>
<p>And secondly, the cynical part of me wonders what kind of academics would make their data and statistical analyses open for public consumption. An easy way for a critic to discredit your research project is to discredit your methods. And using a non-standard model or making an error within your code would be an easy way to destroy a study's findings. Researchers would be taking a large risk by putting themselves in these spotlights.</p>
<p>It's also difficult to know where to draw the line on what the public should know. For example, should the public have open and free reign over video streams from police dashboard and personal cameras? Do we have the right to see the uncensored images coming in from military operations? There are other examples and they demand individual consideration.</p>
<h2>Nonetheless Open Data offers innovation, accountability, and strengthening trust in our systems.</h2>
<p>It is the way forward, and I embrace it.</p>
		</div>
			</div>
		</div>
	</div>
	

<div class="container" >
	<div class="row">
		<div class="col-sm-12" style="padding: 0 2em 1em 0;">
			<span></span></h2><a style="font-size: 25px" href="/posts/2015-03-19-GDG/">DC Google Developer Group - Google + Firebase</a></h2><br>

		<div class="brandcolor2" id="datetag">
		<p> Date: 2015-03-19
	
		|  Tagged:
		
			<em><a href="/tag/data/">data</a></em>
		
		</p>
		<h2>Intro to <a href="https://www.firebase.com/">Firebase</a></h2>
<p><em>The realtime backend for your app.</em><br>
<em>Presented by David East @_davideast</em></p>
<p>David introduced the audience to Firebase by way of thinking through Google documents. The items behind Google docs that the developers tried to optimize included:</p>
<ul>
<li>Speed</li>
<li>Security</li>
<li>Easy authentication</li>
<li>Free hosting</li>
</ul>
<p>Further, what the user expects and what Firebase provides is:</p>
<ul>
<li>Speed</li>
<li>Ability to work offline</li>
<li>Multi-platform experience</li>
<li>Simple authentication (e.g. login with Facebook)</li>
</ul>
<p>You can write an app entirely in Javascript with Firebase and never
worry about a backened. Firebase is cloud hosted, you set up an account
and you have Firebase. It is a NoSQL data-store connected through a
RESTful object.</p>
<p>Whenever data is updated in Firebase it's sent down to the client in
real-time. It persists the data <strong>and</strong> sends it down to all listening
clients.</p>
<p>var ref = new Firebase([put URL from Firebase here/realtime]);
ref.set('text');</p>
<p>Call the set method on the reference that was just created and the
'text' data has been saved to the backend. You can find more of the example online at the Firebase site, but concisely, this is not difficult to set up and run. </p>
<h3>Syncing</h3>
<p>Firebase allows for easy syncing of data.</p>
<p>var label = document.getElementById('label');
var ref = new.Firebase([see above]);
ref.on('value', function(snap) {
  label.innerText = snap.val();
});</p>
<p>A snapshot is returned. Returning data is as easy as calling snap.val().</p>
<h3>Authentication</h3>
<p>Easily authenticate with Google (and since Firebase was purchased by Google this should remain easy for it's remaining lifespan).</p>
<h3>Bindings</h3>
<p>Easily binds together with javascript frameworks:
React, Ember, Angular, Backbone, etc.</p>
<p>Firebase app also allows for easy deployment, organization, and
monirtoring via the command line. Used by GoToMeeting, Warby Parker,
CBS. It is definitely worth considering for real-time apps </p>
		</div>
			</div>
		</div>
	</div>
	

<div class="container" >
	<div class="row">
		<div class="col-sm-12" style="padding: 0 2em 1em 0;">
			<span></span></h2><a style="font-size: 25px" href="/posts/2015-03-12-HadoopPython-meetup/">DC Python Meetup - Hadoop with Python</a></h2><br>

		<div class="brandcolor2" id="datetag">
		<p> Date: 2015-03-12
	
		|  Tagged:
		
			<em><a href="/tag/data/">data</a></em>
		
		</p>
		<p>The following are my notes from the March DC Python meetup on Hadoop, given by Donald P. Miner (@donaldpminer). They might be incomplete. </p>
<h2>What is Hadoop</h2>
<p>This technology derives from Google and reimagines data storage. 
It mostly lives as a Hadoop Distributed File System (HDFS) which: </p>
<ul>
<li>Stores files in folders</li>
<li>Chunks large files into blocks</li>
<li>Duplicates the block three times</li>
<li>Scatters the blocks all over the place</li>
</ul>
<p>Can only create, read, and delete. If you want to edit you can use
something like HBase to run on top on Hadoop such as Acumulo, created by the NSA. The core piece of Hadoop that helps organize the data is MapReduce.</p>
<h2>MapReduce</h2>
<p>Analyzes raw data in HDFS. Jobs are split into Mappers and
Reducers. There are a lot of pieces that 'fall out' from mapping and
reducing. Donald Miner was able to write a book just on the different
things that can be done just with mapping and reducing. <a href="http://shop.oreilly.com/product/0636920025122.do">See MapReduce
design patterns.</a></p>
<h3>Mappers</h3>
<p>The key items of Mappers:</p>
<ul>
<li>Loads data form HDFS</li>
<li>Filter, transform, parse</li>
<li>Outputs (key, value) pairs</li>
</ul>
<h3>Reducers</h3>
<p>The key pieces of Reducers:</p>
<ul>
<li>Automatically groups by the mapper's output key</li>
<li>Aggregate, count, statistics</li>
<li>Outputs to HDFS</li>
</ul>
<p>MapReduce solves batching problems but is not a panacea. Other problems, such as
streaming data, are not appropriate for MapReduce.</p>
<p>Miner's personal workflow is to process in Hadoop and output to CSV or JSON.</p>
<h2>Hadoop Ecosystem</h2>
<p>Hadoop lives within an ecosystem of other projects:</p>
<ul>
<li>Higher level languages like Pig and Hive</li>
<li>HDFS data systems like HBase and Accumulo</li>
<li>Alternative executiion engines like Storm and Spark</li>
<li>Close friends like ZooKeeper, Flume, Avro, Kafka</li>
</ul>
<h3>The one thing I don't like about Hadoop is Java</h3>
<p>But Hadoop with Python is a bit half-baked.</p>
<h2>Cool things</h2>
<p>Data Locality - the idea of processing data where it is instead of
moving it over the network
<em> Linear Scalability
  * HDFS and MapReduce scale linearly
</em> Schema on Read (opposed to schema on write)
  * You load the data first, before you know what you're going to do
    with it. And then the nature of the data, whether it's JSON and what data types there are done later. 
  * You don't have to get rid of the original data!
Example: image analysis. You can have a mapper analyze the metadata
(e.g. who took it) and another to analyze the image. You can even store
it first and then do something with it later.
<em> Transparent Parallelism
  * You don't have to care about a lot of traditional worries like
    scalability, locking,  and threading?
</em> Unstructed Data
  * MapReduce is just Java</p>
<p><em>I can give a devleoper who knows nothing about HDFS a job to run (and
they have no appreciation of what they're doing.)</em></p>
<h2>Python</h2>
<p><strong>I've had long discussions with Java developers and I think it comes
down to a personality preference about why Python over Java.</strong>
Compiled vs. Scripting
With Java I have to compile. I compile to a JAR file, copy that to the
cluster, realize there's a problem, and then come back. </p>
<p>With scripting (python) it's fairly easy to edit-in-place.</p>
<ul>
<li>Python vs. Java</li>
<li>Compiled vs. scripts</li>
<li>Python libraries we all love</li>
<li>World-class libraries for data analysis, etc.</li>
<li>Integration with other things</li>
</ul>
<p><strong>Why Not Python?</strong></p>
<ul>
<li>
<p>You might be on your own in some cases with downloading, installing</p>
</li>
<li>
<p>Smaller community, almost no official support</p>
</li>
</ul>
<h2>Survey of Python Hadoop things</h2>
<p><strong>MrJob</strong>
Wrape Hadoop streaming that wraps the MapReduce processes (used and
maintained by Yelp). Well-documented and can run locally in Amazon
Elastic MapReduce (EMR),
or Hadoop.</p>
<p><strong>Pydoop</strong>
You can write MapReduce jobs in Python. Uses C++ Pipes which should be
faster than wrapping streaming.</p>
<p>See Clouderas blogpost guide to python frameworks for Hadoop</p>
<p><strong>Pig</strong>
Does data flow transformation. A higher level platform and language for
analyzing data that happens to run MapReduce underneath.
Book: Agile Data Science - not terribly useful but fun to read</p>
<p><strong>Snakebite</strong>
Big missing part: cannot write data.</p>
<p>(See Luigi from Spotify, see the Spotify open source projects)</p>
<p><strong>HBase</strong>
Not really there yet and has failed to gain community momentum. Java is
still King.</p>
<h2>The Future</h2>
<p><strong>Spark</strong>
Spark generally is a lot faster and easier to write than MapReduce. It provides a high-level API in Scala, Java, and Python that
makes parallel jobs easy to write. <strong>PySpark</strong> 
Some folks say that Spark is better than MapReduce
RDDs = Resilient Distributed Datasets
RDDs are kept in memory for the most part.
Spark does the computations in memory as opposed to MapReduce. But if
your dataset gets too large it writes to disk. Get get comfortable with
lambdas.</p>
<h2>Update, see the video of the talk below</h2>
<iframe width="560" height="315" src="https://www.youtube.com/embed/g99U7c4jSNs" frameborder="0" allowfullscreen></iframe>
		</div>
			</div>
		</div>
	</div>
	

<div class="container" >
	<div class="row">
		<div class="col-sm-12" style="padding: 0 2em 1em 0;">
			<span></span></h2><a style="font-size: 25px" href="/posts/2015-03-17-AppswWebsockets/">DC Django-district Meetup - Python Websockets</a></h2><br>

		<div class="brandcolor2" id="datetag">
		<p> Date: 2015-03-17
	
		|  Tagged:
		
			<em><a href="/tag/data/">data</a></em>
		
		</p>
		<p>The following are my notes from the March DC Django-district meetup on websockets with python, presented by Matt Makai (@mattmakai) of Twilio. Matt Makai also maintains <a href="http://www.fullstackpython.com/"><strong>Full Stack Python</strong></a>.</p>
<p><em>Note: These notes might be incomplete.</em></p>
<h3><a href="http://www.mattmakai.com/presentations/2015-async-web-apps-django-district.html#/">Asynchronous Python Web Apps with WebSockets</a></h3>
<p>To summarize, the core idea for the talk was:</p>
<p><em>If you want full communication between the browser and the server this
is how to do it without resorting to Node.js</em></p>
<p>The idea is to get a WSGI server to invoke the python code you've
written. WSGI is a blocking interface, meaning that every time a request
arrives the server will process the request and be done. In the context
of the historical world wide web this made sense.</p>
<h3>The old days</h3>
<p>In the beginning, the web browser communicates with the web server. The server would
simple return back static files and that's ok <em>if you only have static
files.</em></p>
<p>But if you want dynamic applications you need something more - the WSGI
server interface. When you run a Flask or Django application the web
server, WSGI, acts as a proxy - it runs some python and returns back
something: HTML, JSON, etc. </p>
<p>Around 2005 AJAX was introduced, giving us richer applications. In this
regime we are able to retrieve data from the backend: <a href="http://en.wikipedia.org/wiki/Push_technology">Long Polling.</a> E.g. Having Facebook pdates coming in to your feed. However, this was problematic and the web community responded with:</p>
<h3>Async</h3>
<p>Async can keep the communication open between the client and the server.
Before we had Long Polling with AJAX (a constant checking for updates).
But we wanted a way of passing data between server and browser. Async
keeps that connection open.</p>
<p>There are differetn ways of doing this - we've usually resorted to
Node.js but we can use python solutions, like <strong>gevent</strong></p>
<h3>Live Coding Example</h3>
<p><strong>See python-websockets-example from Github</strong></p>
<p>The libraries being used:</p>
<ul>
<li>Flask</li>
<li>Flask-socketio (written by person who wrote O'Reilly Flask book)</li>
<li>Redis</li>
</ul>
<p>Create an empty repository, install those libraries, and then</p>
<p>pip freeze </p>
<p>the dependencies </p>
<p>The python code for the older style of application:</p>
<p>import redis
from flask import Flask, render_template</p>
<p>app = Flask(<strong>name</strong>)
db = redis.StrictRedis('localhost', 6379, 0)</p>
<p>@app.route('/')
def main():
  c = db.incr('user_count')
  return render_template('main.html', counter=c)</p>
<p>if <strong>name</strong> == "<strong>main</strong>":
  app.run(debug=True)</p>
<p>And then we code custom to render our python script (<em><a href="https://github.com/makaimc/python-websockets-example">from here on out
you should refer to Matt's repository for the code, I won't copy and
paste it.</a></em>)</p>
<p><a href="https://github.com/makaimc/python-websockets-example/blob/master/templates/main.html">The HTML template.</a></p>
<p>Ok, so the example works but we can improve it using Async libraries.</p>
<h2>Let's do it with Socketio</h2>
<p>Lot of code but essentially we submit the form as SocketIo and not HTML.</p>
<p>The server is getting information and serving it out, but now we can use
socketio.emit to have the server send it out to all other web browsers
connecting.</p>
<p>Matt is presenting at this year's <a href="https://us.pycon.org/2015/schedule/">pycon in Montreal</a> and I'll be
sure to post his talk here when it's complete.</p>
		</div>
			</div>
		</div>
	</div>
	

<div class="container" >
	<div class="row">
		<div class="col-sm-12" style="padding: 0 2em 1em 0;">
			<span></span></h2><a style="font-size: 25px" href="/posts/2015-04-06-IncubatorWin/">A win at the Incubator Pitch Fest</a></h2><br>

		<div class="brandcolor2" id="datetag">
		<p> Date: 2015-04-06
	
		|  Tagged:
		
			<em><a href="/tag/data/">data</a></em>
		
			<em><a href="/tag/python/">python</a></em>
		
			<em><a href="/tag/flask/">flask</a></em>
		
		</p>
		<p>For the past three months I've been working nights and weekends on a
data product for the</p>
<h3><a href="http://www.districtdatalabs.com/#!incubator/c185j">District Data Labs (DDL) Incubator Program</a></h3>
<p><em>Which you should totally participate in if you're in the D.C. area and
even marginally interested in data science.</em></p>
<p>Well, this past weekend my team successfully pitched the winning data
product. You can find the app below <a href="//dew.herokuapp.com">or live here</a>.</p>
<iframe src="//dew.herokuapp.com" width="800px;" height="300px;"></iframe>

<p>Winning is great, but applying programming and visualization techniques I've been learning - and working on meaningful a project - is where I find fulfillment.</p>
		</div>
			</div>
		</div>
	</div>
	

<div class="container" >
	<div class="row">
		<div class="col-sm-12" style="padding: 0 2em 1em 0;">
			<span></span></h2><a style="font-size: 25px" href="/posts/2015-06-01-HealthDataPaloozaTutorial%20copy/">Finding top opioid prescribers in the U.S., an Open Data Science Tutorial</a></h2><br>

		<div class="brandcolor2" id="datetag">
		<p> Date: 2015-06-01
	
		|  Tagged:
		
			<em><a href="/tag/python/">python</a></em>
		
			<em><a href="/tag/data%20science/">data science</a></em>
		
			<em><a href="/tag/data/">data</a></em>
		
			<em><a href="/tag/health%20data/">health data</a></em>
		
		</p>
		<p><em>Scroll to the bottom of the page if you want to get right to the python code and totals for the first CSV of data.</em> </p>
<h3>Background and Introduction</h3>
<p>The <strong>Centers for Medicare and Medicaid Services (CMS)</strong> recently released a <a href="http://www.cms.gov/Research-Statistics-Data-and-Systems/Statistics-Trends-and-Reports/Medicare-Provider-Charge-Data/Part-D-Prescriber.html">massive dataset</a> on the Part D program containing
data on drug amounts, their cost, how many beneficiaries and claims were involved
and the <em>names</em> and locations of the physicians prescribing these medications.</p>
<p>In the spirit of the open data movement, and in time for the <a href="http://healthdatapalooza.org/">2015 Health Data Palooza conference</a>, this tutorial series proposes to total and visualize these data. The <a href="https://github.com/wsankey/PartDAnalysis">code</a> will be made available for anyone to download and modify. This tutorial uses <em>free</em> open source software so you can replicate these findings even if you are using your wife's old macbook air over the weekend. </p>
<h4>Open Data Science with Python</h4>
<p>This tutorial uses Python and eventually Javascript. I will be going through this using a standard text editor and the resources available via the Mac terminal. I am using Python 2.7 along with the Pandas module (all free). If you have a PC you can follow along but the screens below will look different and the steps needed to get this running may take some effort to configure. </p>
<p><strong>Note: These tutorials may be a struggle to someone new to Python, Javascript, and the web world generally. Proceed with caution, and see other
resources for more help.</strong></p>
<h3>Results first</h3>
<p>By the end of this tutorial you'll be able to query the top N prescribers of opioids in 2013. Below I've pulled the top 5 by total drug cost (right screen0) from the first CSV, my code is on the left:
<img class="parent" src="/static/media/top5_prescribers.png">
<em>Bigger screenshots are provided below</em></p>
<h3>Find, download and explore the data</h3>
<p>For this first tutorial we'll total drug costs for one of the CSVs. Later we'll download and perform this operation over <em>all</em> the CSVs. Download the first workbook (Aa to AI) open and save it as a CSV. There are some ways to automate this using Python--and in the futre we'll attempt a script that completes this for us. </p>
<h4>Understanding what we have</h4>
<p>Let's take a breather, back up, and try and understand what we are looking at. After saving the data in our folder we can start writing some python scripts to look and manipulate it. </p>
<p>In our python code we import the Pandas module and use it to look at the first ten observations. 
<pre><code class="language-python">
import pandas as pd</p>
<h1>Let's read in the CSV and retain the columns we're interested in.</h1>
<p>df = pd.read_csv("PartD_Prescriber_PUF_NPI_DRUG_Aa_AL_CY2013.csv", nrows=100)
print df.head(10)
</code></pre>In running this code we'll be able to get a sense of this dataset, the variable names and what we'll need to do to transform it into something we can use. </p>
<p>From this investigation we find some key variables: the prescriber's NPI (a unique identifier), their first and last name along with their city and state, the drug's total cost, and the name of the generic drugs. </p>
<h4>What drugs should we investigate. What are opioids?</h4>
<p>We need a list of the generic drugs of interest. After some simple internet sleuthing we find this <a href="http://www.fda.gov/Drugs/DrugSafety/InformationbyDrugClass/ucm251735.htm">list of opioid generics from the FDA.</a> The opioid generics identified in this list include Fentanyl, Methadone Hydrochloride (HCL), Morphine Sulfate, and Oxymorphone HCL. </p>
<h3>Finding totals by provider</h3>
<p>In the interest of time I've provided below the final form of the code. Essentially, after reading our CSV into the Pandas module we identify the opioid generics of interest, we clean the total drug cost variable, and then we sum by the unique provider identifier (NPI). Once we have those totals we write that information to the 'top_prescribers_in_first_file' CSV. I've commented out the code below to offer some insight into what's happening at each step and please let me know in the comments if you'd like more information on anything going on below.
<pre><code class="language-python">
import pandas as pd</p>
<h1>Let's read in the CSV and retain the columns we're interested in.</h1>
<p>df = pd.read_csv("PartD_Prescriber_PUF_NPI_DRUG_Aa_AL_CY2013.csv")</p>
<p>df = df[['npi', 'total_drug_cost', 'nppes_provider_last_org_name', 'nppes_provider_first_name', 'nppes_provider_city', 'nppes_provider_state', 'generic_name']]</p>
<h1>We identify the following generics as opioids.</h1>
<p>def opiods(series):
    if series["generic_name"]=="FENTANYL":
        return 1
    elif series["generic_name"]=="METHADONE HCL":
        return 1
    elif series["generic_name"]=="MORPHINE SULFATE":
        return 1
    elif series["generic_name"]=="OXYMORPHONE HCL":
        return 1
    else:
        return 0</p>
<h1>The opioids function is applied and the opioids retained.</h1>
<p>df['opioid_var'] = df.apply(opiods, axis=1)
df = df.loc[(df.opioid_var==True)]</p>
<h1>Our total drug cost column is messy and needs cleaning.</h1>
<p>df['total_drug_cost'] = df['total_drug_cost'].fillna(0)
df['total_drug_cost'] = df['total_drug_cost'].str.strip("$")
df['total_drug_cost'] = df['total_drug_cost'].str.replace(',','')
df['total_drug_cost'] = df['total_drug_cost'].astype(float)</p>
<h1>Group and sum by provider NPI, sort the summed column in descending order and send our dataset to a CSV file.</h1>
<p>df['tot'] = df['total_drug_cost'].groupby(df['npi']).transform('sum')
df = df.drop_duplicates(cols='npi')</p>
<p>top100 = df.sort('tot', ascending=False)[:100]
top100 = top100[['npi', 'tot', 'nppes_provider_last_org_name', 'nppes_provider_first_name', 'nppes_provider_city', 'nppes_provider_state']]</p>
<p>top100.to_csv('top_prescribers_in_first_file.csv')</p>
<p></code></pre></p>
<h3>Results</h3>
<p>The output CSV returns the top 100 prescribers of opioids by total drug cost in 2013. We find the very top prescriber in this dataset prescriber $798,501 worth of opioids and the top 20 prescribers in this first file prescribed over $100,000 worth of opioids.</p>
<p><img class="parent" src="/static/media/top100excel.png"></p>
<p><em>Important note: There many be important factors about a prescriber's population (e.g. having more patients with cancer or in hospice) that are behind these totals. This basic analysis does not speak to any of those particular factors.</em> </p>
<h3>Next steps</h3>
<p>Ok, we've done this for one of our worksheets but we have more to do. The next tutorial will show you how to loop through the CSVs we have and save the top 100 prescribers from each. From there we'll use the mindset of the <a href="http://en.wikipedia.org/wiki/Merge_sort">mergesort alogrithmn</a> to find
the top 100 prescribers of opioids in the country. Finally, in the last tutorial we'll take this data and visualize their totals. </p>
<p>Stay tuned.</p>
		</div>
			</div>
		</div>
	</div>
	

	
		</div>
	</div>
</div>

    </div>
  </div>
</div>
<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-67546347-1', 'auto');
  ga('send', 'pageview');

</script>
</body>
</html>